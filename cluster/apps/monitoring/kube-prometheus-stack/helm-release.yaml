---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 12h
  maxHistory: 2
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 31.0.2
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
  install:
    crds: Create
  upgrade:
    crds: CreateReplace
  values:
    fullnameOverride: prom
    alertmanager:
      config:
        global:
          slack_api_url: ${ALERTMANAGER_WEBHOOK}
          resolve_timeout: 5m
        receivers:
          - name: "null"
          - name: slack-notifications
            slack_configs:
              - channel: "#prometheus-alerts"
                icon_url: https://avatars3.githubusercontent.com/u/3380462
                username: Alertmanager
                send_resolved: true
                color: '{{ template "slack.color" . }}'
                title: '{{ template "slack.title" . }}'
                text: '{{ template "slack.text" . }}'
                actions:
                  - type: button
                    text: "Runbook :green_book:"
                    url: "{{ (index .Alerts 0).Annotations.runbook_url }}"
                  - type: button
                    text: "Query :mag:"
                    url: "{{ (index .Alerts 0).GeneratorURL }}"
                  - type: button
                    text: "Dashboard :chart_with_upwards_trend:"
                    url: "{{ (index .Alerts 0).Annotations.dashboard_url }}"
                  - type: button
                    text: "Silence :no_bell:"
                    url: '{{ template "__alert_silence_link" . }}'
        route:
          group_by:
            - alertname
            - job
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 6h
          receiver: slack-notifications
          routes:
            - matchers:
                - alertname =~ Watchdog|KubeAPILatencyHigh|CPUThrottlingHigh|AggregatedAPIErrors|AggregatedAPIDown|KubeAPIErrorBudgetBurn|etcdHighNumberOfFailedGRPCRequests
              receiver: "null"
            - matchers:
                - severity =~ critical|warning
              receiver: slack-notifications
              continue: true
      templateFiles:
        notifications.tmpl: |-
          {{ define "__alert_silence_link" -}}
              {{ .ExternalURL }}/#/silences/new?filter=%7B
              {{- range .CommonLabels.SortedPairs -}}
                  {{- if ne .Name "alertname" -}}
                      {{- .Name }}%3D"{{- .Value -}}"%2C%20
                  {{- end -}}
              {{- end -}}
              alertname%3D"{{- .CommonLabels.alertname -}}"%7D
          {{- end }}

          {{ define "__alert_severity" -}}
              {{- if eq .CommonLabels.severity "critical" -}}
              *Severity:* `Critical`
              {{- else if eq .CommonLabels.severity "warning" -}}
              *Severity:* `Warning`
              {{- else if eq .CommonLabels.severity "info" -}}
              *Severity:* `Info`
              {{- else -}}
              *Severity:* :question: {{ .CommonLabels.severity }}
              {{- end }}
          {{- end }}

          {{ define "slack.title" -}}
            [{{ .Status | toUpper -}}
            {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
            ] {{ .CommonLabels.alertname }}
          {{- end }}

          {{ define "slack.text" -}}
            {{ template "__alert_severity" . }}
            {{- if (index .Alerts 0).Annotations.summary }}
            {{- "\n" -}}
            *Summary:* {{ (index .Alerts 0).Annotations.summary }}
            {{- end }}
            {{ range .Alerts }}
              {{- if .Annotations.description }}
              {{- "\n" -}}
              {{ .Annotations.description }}
              {{- "\n" -}}
              {{- end }}
              {{- if .Annotations.message }}
              {{- "\n" -}}
              {{ .Annotations.message }}
              {{- "\n" -}}
              {{- end }}
            {{- end }}
          {{- end }}

          {{ define "slack.color" -}}
            {{ if eq .Status "firing" -}}
              {{ if eq .CommonLabels.severity "warning" -}}
                  warning
              {{- else if eq .CommonLabels.severity "critical" -}}
                  danger
              {{- else -}}
                  #439FE0
              {{- end -}}
            {{ else -}}
            good
            {{- end }}
          {{- end }}
      podDisruptionBudget:
        enabled: true
      alertmanagerSpec:
        externalUrl: https://alertmanager.${ROOT_DOMAIN}
        replicas: 2
        podAntiAffinity: hard
        storage:
          volumeClaimTemplate:
            metadata:
              name: data
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
    kubelet:
      enabled: true
    coreDns:
      enabled: true
    kubeEtcd:
      enabled: true
      serviceMonitor:
        scheme: https
        serverName: localhost
        caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    kubeProxy:
      enabled: false
    prometheus:
      podDisruptionBudget:
        enabled: false
      prometheusSpec:
        retention: 30d
        externalUrl: https://prometheus.${ROOT_DOMAIN}
        externalLabels:
          cluster: mcf-k8s-cluster
        portName: http-web
        logFormat: json
        replicas: 1
        secrets:
          - etcd-client-cert
        # To ensure proper scheduling, set the CPU requests to 0.25 cores; Additionally remove the cpu limits - a side effect of smaller platforms is the CPU scheduler
        # tends to aggressivly throttle prometheus, running guaranteed QoS of 1 or 2 cores results in nearly 50-100% throttling. At that point, there's not a good reason
        # to set the CPU limits any longer.
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 3096Mi
        nodeSelector:
          kubernetes.io/arch: amd64
        storageSpec:
          volumeClaimTemplate:
            metadata:
              name: data
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 80Gi
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
    prometheusOperator:
      admissionWebhooks:
        certManager:
          enabled: true
      # Setting this option to 0 to disable cpu limits
      # see https://github.com/prometheus-operator/prometheus-operator/blob/master/cmd/operator/main.go#L175
      configReloaderCpu: 0
    grafana:
      enabled: false
      forceDeployDatasources: true
      forceDeployDashboards: true
      sidecar:
        datasources:
          url: http://prom-prometheus.monitoring:9090/
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              sourceLabels:
                - __meta_kubernetes_endpoint_node_name
              targetLabel: node
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              sourceLabels:
                - __meta_kubernetes_endpoint_node_name
              targetLabel: node
