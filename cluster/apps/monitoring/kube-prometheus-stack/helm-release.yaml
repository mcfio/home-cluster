---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 6h
  maxHistory: 2
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 18.0.8
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
  install:
    crds: Create
  upgrade:
    crds: CreateReplace
  values:
    fullnameOverride: prom
    defaultRules:
      # for initial deployment, disabling the creation of default rules
      create: true
      # fine grained rule control
      rules:
        alertmanager: false
        etcd: true
        general: false
        k8s: true
        kubeApiserver: false
        kubeApiserverAvailability: false
        kubeApiserverError: false
        kubeApiserverSlos: false
        kubelet: false
        kubePrometheusGeneral: false
        kubePrometheusNodeAlerting: false
        kubePrometheusNodeRecording: false
        kubernetesAbsent: false
        kubernetesApps: false
        kubernetesResources: false
        kubernetesStorage: false
        kubernetesSystem: false
        kubeScheduler: false
        kubeStateMetrics: false
        network: false
        node: true
        prometheus: false
        prometheusOperator: false
        time: false
    alertmanager:
      config:
        global:
          slack_api_url: ${ALERTMANAGER_WEBHOOK}
          resolve_timeout: 5m
        receivers:
        - name: "null"
        - name: slack-notifications
          slack_configs:
          - channel: "#prometheus-alerts"
            icon_url: https://avatars3.githubusercontent.com/u/3380462
            username: Alertmanager
            send_resolved: true
            title: '{{ template "custom_title" . }}'
            text: '{{ template "custom_slack_message" . }}'
        route:
          group_by:
          - alertname
          - job
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 6h
          receiver: slack-notifications
          routes:
          - match:
              alertname: Watchdog
            receiver: "null"
          - match_re:
              severity: critical|warning
            receiver: slack-notifications
            continue: true
          - match:
              alertname: etcdHighNumberOfFailedGRPCRequests
            receiver: "null"
      templateFiles:
        notifications.tmpl: |-
          {{ define "__single_message_title" }}{{ range .Alerts.Firing }}{{ .Labels.alertname }} @ {{ .Annotations.identifier }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }} @ {{ .Annotations.identifier }}{{ end }}{{ end }}

          {{ define "custom_title" }}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}{{ template "__single_message_title" . }}{{ end }}{{ end }}

          {{ define "custom_slack_message" }}
          {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
          {{ range .Alerts.Firing }}{{ .Annotations.description }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.description }}{{ end }}
          {{ else }}
          {{ if gt (len .Alerts.Firing) 0 }}
          *Alerts Firing:*
          {{ range .Alerts.Firing }}- {{ .Annotations.identifier }}: {{ .Annotations.description }}
          {{ end }}{{ end }}
          {{ if gt (len .Alerts.Resolved) 0 }}
          *Alerts Resolved:*
          {{ range .Alerts.Resolved }}- {{ .Annotations.identifier }}: {{ .Annotations.description }}
          {{ end }}{{ end }}
          {{ end }}
          {{ end }}
      podDisruptionBudget:
        enabled: true
      alertmanagerSpec:
        externalUrl: https://alertmanager.${ROOT_DOMAIN}
        replicas: 2
        podAntiAffinity: hard
        storage:
          volumeClaimTemplate:
            metadata:
              name: data
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
    kukbelet:
      enabled: true
    kubeControllerManager:
      enabled: true
      service:
        port: 10257
        targetPort: 10257
      serviceMonitor:
        https: true
        servername: localhost
        insecureSkipVerify: true
    coreDns:
      enabled: true
    kubeEtcd:
      enabled: true
      serviceMonitor:
        scheme: https
        serverName: localhost
        caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    kubeScheduler:
      enabled: true
      service:
        port: 10259
        targetPort: 10259
      serviceMonitor:
        https: true
        serverName: localhost
        insecureSkipVerify: true
    kubeProxy:
      enabled: false
    prometheus:
      podDisruptionBudget:
        enabled: false
      prometheusSpec:
        retention: 30d
        externalUrl: https://prometheus.${ROOT_DOMAIN}
        externalLabels:
          cluster: mcf-k8s-cluster
        portName: http-web
        logFormat: json
        replicas: 1
        secrets:
        - etcd-client-cert
        # To ensure proper scheduling, set the CPU requests to 0.25 cores; Additionally remove the cpu limits - a side effect of smaller platforms is the CPU scheduler
        # tends to aggressivly throttle prometheus, running guaranteed QoS of 1 or 2 cores results in nearly 50-100% throttling. At that point, there's not a good reason
        # to set the CPU limits any longer.
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 3096Mi
        storageSpec:
          volumeClaimTemplate:
            metadata:
              name: data
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 80Gi
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
    prometheusOperator:
      admissionWebhooks:
        certManager:
          enabled: true
      # Setting this option to 0 to disable cpu limits
      # see https://github.com/prometheus-operator/prometheus-operator/blob/master/cmd/operator/main.go#L175
      configReloaderCpu: 0
    grafana:
      enabled: false
      forceDeployDatasources: true
      forceDeployDashboards: true
      sidecar:
        datasources:
          url: http://prom-prometheus.monitoring:9090/
    prometheus-node-exporter:
      fullnameOverride: node-exporter
    kubeStateMetrics:
      enabled: true
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
