---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 6h
  maxHistory: 2
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 18.0.2
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
  install:
    crds: Create
  upgrade:
    crds: CreateReplace
  values:
    fullnameOverride: prom
    defaultRules:
      # for initial deployment, disabling the creation of default rules
      create: true
      # fine grained rule control
      rules:
        alertmanager: false
        etcd: true
        general: false
        k8s: true
        kubeApiserver: false
        kubeApiserverAvailability: false
        kubeApiserverError: false
        kubeApiserverSlos: false
        kubelet: false
        kubePrometheusGeneral: false
        kubePrometheusNodeAlerting: false
        kubePrometheusNodeRecording: false
        kubernetesAbsent: false
        kubernetesApps: false
        kubernetesResources: false
        kubernetesStorage: false
        kubernetesSystem: false
        kubeScheduler: false
        kubeStateMetrics: false
        network: false
        node: true
        prometheus: false
        prometheusOperator: false
        time: false
    alertmanager:
      config:
        receivers:
        - name: gitlab
          webhook_configs:
          - http_config:
              bearer_token: 58ce824359c24da289cdda3bbb364fc3
            send_resolved: true
            url: https://gitlab.com/mcfaul.cloud/GitOps/home-cluster/prometheus/alerts/notify.json
        - name: "null"
        route:
          group_by:
          - alertname
          - job
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 6h
          receiver: gitlab
          routes:
          - receiver: "null"
            match:
              alertname: Watchdog
          - receiver: gitlab
            match_re:
              severity: critical|warning
            continue: true
          - match:
              alertname: etcdHighNumberOfFailedGRPCRequests
            receiver: "null"
      podDisruptionBudget:
        enabled: true
      alertmanagerSpec:
        externalUrl: https://alertmanager.milton.mcf.io
        portName: http-web
        logFormat: json
        replicas: 2
        podAntiAffinity: hard
        storage:
          volumeClaimTemplate:
            metadata:
              name: data
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
    kukbelet:
      enabled: true
    kubeControllerManager:
      enabled: true
      service:
        port: 10257
        targetPort: 10257
      serviceMonitor:
        https: true
        servername: localhost
        insecureSkipVerify: true
    coreDns:
      enabled: true
    kubeEtcd:
      enabled: true
      serviceMonitor:
        scheme: https
        serverName: localhost
        caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    kubeScheduler:
      enabled: true
      service:
        port: 10259
        targetPort: 10259
      serviceMonitor:
        https: true
        serverName: localhost
        insecureSkipVerify: true
    kubeProxy:
      enabled: false
    prometheus:
      podDisruptionBudget:
        enabled: false
      prometheusSpec:
        retention: 30d
        externalUrl: https://prometheus.milton.mcf.io
        externalLabels:
          cluster: mcf-k8s-cluster
        portName: http-web
        logFormat: json
        replicas: 1
        secrets:
        - etcd-client-cert
        # To ensure proper scheduling, set the CPU requests to 0.25 cores; Additionally remove the cpu limits - a side effect of smaller platforms is the CPU scheduler
        # tends to aggressivly throttle prometheus, running guaranteed QoS of 1 or 2 cores results in nearly 50-100% throttling. At that point, there's not a good reason
        # to set the CPU limits any longer.
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 3096Mi
        storageSpec:
          volumeClaimTemplate:
            metadata:
              name: data
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 80Gi
        # thanos:
        #   image: raspbernetes/thanos:v0.22.0
        #   version: v0.22.0
        #   objectStorageConfig:
        #     name: thanos-objstore-secret
        #     key: objstore.yml
        #   resources:
        #     requests:
        #       memory: 128Mi
        #       cpu: 100m
        #     # As noted above, this deployment will not set the cpu limits for the container
        #     limits:
        #       memory: 256Mi
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
      # thanosService:
      #   enabled: true
    prometheusOperator:
      admissionWebhooks:
        certManager:
          enabled: true
      # Setting this option to 0 to disable cpu limits
      # see https://github.com/prometheus-operator/prometheus-operator/blob/master/cmd/operator/main.go#L175
      configReloaderCpu: 0
    grafana:
      enabled: false
      forceDeployDatasources: true
      forceDeployDashboards: true
      sidecar:
        datasources:
          url: http://prom-prometheus.monitoring:9090/
    prometheus-node-exporter:
      fullnameOverride: node-exporter
    kubeStateMetrics:
      enabled: true
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
